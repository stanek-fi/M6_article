@inproceedings{baiHowImportantTrainValidation2021,
  title     = {How {{Important}} Is the {{Train-Validation Split}} in {{Meta-Learning}}?},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author    = {Bai, Yu and Chen, Minshuo and Zhou, Pan and Zhao, Tuo and Lee, Jason and Kakade, Sham and Wang, Huan and Xiong, Caiming},
  year      = {2021},
  pages     = {543--553},
  publisher = {{PMLR}},
  keywords  = {GFM,read},
  file      = {C:\Users\stane\Zotero\storage\2A4PXCPI\Bai et al (2021) - How Important is the Train-Validation Split in Meta-Learning.pdf}
}

@article{bandaraImprovingAccuracyGlobal2021,
  title    = {Improving the Accuracy of Global Forecasting Models Using Time Series Data Augmentation},
  author   = {Bandara, Kasun and Hewamalage, Hansika and Liu, Yuan-Hao and Kang, Yanfei and Bergmeir, Christoph},
  year     = {2021},
  month    = dec,
  journal  = {Pattern Recognition},
  volume   = {120},
  pages    = {108148},
  issn     = {0031-3203},
  doi      = {10.1016/j.patcog.2021.108148},
  urldate  = {2021-11-25},
  abstract = {Forecasting models that are trained across sets of many time series, known as Global Forecasting Models (GFM), have shown recently promising results in forecasting competitions and real-world applications, outperforming many state-of-the-art univariate forecasting techniques. In most cases, GFMs are implemented using deep neural networks, and in particular Recurrent Neural Networks (RNN), which require a sufficient amount of time series to estimate their numerous model parameters. However, many time series databases have only a limited number of time series. In this study, we propose a novel, data augmentation based forecasting framework that is capable of improving the baseline accuracy of the GFM models in less data-abundant settings. We use three time series augmentation techniques: GRATIS, moving block bootstrap (MBB), and dynamic time warping barycentric averaging (DBA) to synthetically generate a collection of time series. The knowledge acquired from these augmented time series is then transferred to the original dataset using two different approaches: the pooled approach and the transfer learning approach. When building GFMs, in the pooled approach, we train a model on the augmented time series alongside the original time series dataset, whereas in the transfer learning approach, we adapt a pre-trained model to the new dataset. In our evaluation on competition and real-world time series datasets, our proposed variants can significantly improve the baseline accuracy of GFM models and outperform state-of-the-art univariate forecasting methods.},
  langid   = {english},
  keywords = {GFM,read},
  file     = {C:\Users\stane\Zotero\storage\XH5FR427\Bandara et al (2021) - Improving the accuracy of global forecasting models using time series data augmentation.pdf}
}

@inproceedings{beckHypernetworksMetaReinforcementLearning2023,
  title     = {Hypernetworks in {{Meta-Reinforcement Learning}}},
  booktitle = {Proceedings of {{The}} 6th {{Conference}} on {{Robot Learning}}},
  author    = {Beck, Jacob and Jackson, Matthew Thomas and Vuorio, Risto and Whiteson, Shimon},
  year      = {2023},
  month     = mar,
  pages     = {1478--1487},
  publisher = {{PMLR}},
  issn      = {2640-3498},
  urldate   = {2023-09-11},
  abstract  = {Training a reinforcement learning (RL) agent on a real-world robotics task remains generally impractical due to sample inefficiency. Multi-task RL and meta-RL aim to improve sample efficiency by generalizing over a distribution of related tasks. However, doing so is difficult in practice: In multi-task RL, state of the art methods often fail to outperform a degenerate solution that simply learns each task separately. Hypernetworks are a promising path forward since they replicate the separate policies of the degenerate solution while also allowing for generalization across tasks, and are applicable to meta-RL. However, evidence from supervised learning suggests hypernetwork performance is highly sensitive to the initialization. In this paper, we 1) show that hypernetwork initialization is also a critical factor in meta-RL, and that naive initializations yield poor performance; 2) propose a novel hypernetwork initialization scheme that matches or exceeds the performance of a state-of-the-art approach proposed for supervised settings, as well as being simpler and more general; and 3) use this method to show that hypernetworks can improve performance in meta-RL by evaluating on multiple simulated robotics benchmarks.},
  langid    = {english},
  keywords  = {GFM,skimmed},
  file      = {C\:\\Users\\stane\\Zotero\\storage\\9F2WFQYX\\Beck et al (2023) - Hypernetworks in Meta-Reinforcement Learning.pdf;C\:\\Users\\stane\\Zotero\\storage\\NSLEIMHT\\Beck et al. - 2023 - Hypernetworks in Meta-Reinforcement Learning.pdf}
}

@article{brockSMASHOneShotModel2017,
  title         = {{{SMASH}}: {{One-Shot Model Architecture Search}} through {{HyperNetworks}}},
  shorttitle    = {{{SMASH}}},
  author        = {Brock, Andrew and Lim, Theodore and Ritchie, J. M. and Weston, Nick},
  year          = {2017},
  month         = aug,
  journal       = {arXiv:1708.05344 [cs]},
  eprint        = {1708.05344},
  primaryclass  = {cs},
  urldate       = {2022-05-09},
  abstract      = {Designing architectures for deep neural networks requires expert knowledge and substantial computation time. We propose a technique to accelerate architecture selection by learning an auxiliary HyperNet that generates the weights of a main model conditioned on that model's architecture. By comparing the relative validation performance of networks with HyperNet-generated weights, we can effectively search over a wide range of architectures at the cost of a single training run. To facilitate this search, we develop a flexible mechanism based on memory read-writes that allows us to define a wide range of network connectivity patterns, with ResNet, DenseNet, and FractalNet blocks as special cases. We validate our method (SMASH) on CIFAR-10 and CIFAR-100, STL-10, ModelNet10, and Imagenet32x32, achieving competitive performance with similarly-sized hand-designed networks. Our code is available at https://github.com/ajbrock/SMASH},
  archiveprefix = {arxiv},
  keywords      = {GFM,read},
  file          = {C\:\\Users\\stane\\Zotero\\storage\\8UHT96EW\\Brock et al. - 2017 - SMASH One-Shot Model Architecture Search through .pdf;C\:\\Users\\stane\\Zotero\\storage\\R7GRGCHI\\1708.html}
}

@incollection{duncanForecastingAnalogousTime2001,
  title     = {Forecasting {{Analogous Time Series}}},
  booktitle = {Principles of {{Forecasting}}: {{A Handbook}} for {{Researchers}} and {{Practitioners}}},
  author    = {Duncan, George T. and Gorr, Wilpen L. and Szczypula, Janusz},
  editor    = {Armstrong, J. Scott},
  year      = {2001},
  series    = {International {{Series}} in {{Operations Research}} \& {{Management Science}}},
  pages     = {195--213},
  publisher = {{Springer US}},
  address   = {{Boston, MA}},
  doi       = {10.1007/978-0-306-47630-3_10},
  urldate   = {2021-11-25},
  abstract  = {Organizations that use time-series forecasting regularly, generally use it for many products or services. Among the variables they forecast are groups of analogous time series (series that follow similar, time-based patterns). Their covariation is a largely untapped source of information that can improve forecast accuracy. We take the Bayesian pooling approach to drawing information from analogous time series to model and forecast a given time series. In using Bayesian pooling, we use data from analogous time series as multiple observations per time period in a group-level model. We then combine estimated parameters of the group model with conventional time-series-model parameters, using so-called weights shrinkage. Major benefits of this approach are that it (1) requires few parameters for estimation; (2) builds directly on conventional time-series models; (3) adapts to pattern changes in time series, providing rapid adjustments and accurate model estimates; and (4) screens out adverse effects of outlier data points on time-series model estimates. For practitioners, we provide the terms, concepts, and methods necessary for a basic understanding of Bayesian pooling and the conditions under which it improves upon conventional time-series methods. For researchers, we describe the experimental data, treatments, and factors needed to compare the forecast accuracy of pooling methods. Last, we present basic principles for applying pooling methods and supporting empirical results. Conditions favoring pooling include time series with high volatility and outliers. Simple pooling methods are more accurate than complex methods, and we recommend manual intervention for cases with few time series.},
  isbn      = {978-0-306-47630-3},
  langid    = {english},
  keywords  = {GFM,read},
  file      = {C:\Users\stane\Zotero\storage\LPHMAFC3\Duncan et al (2001) - Forecasting Analogous Time Series.pdf}
}

@inproceedings{finnModelAgnosticMetaLearningFast2017,
  title     = {Model-{{Agnostic Meta-Learning}} for {{Fast Adaptation}} of {{Deep Networks}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author    = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  year      = {2017},
  month     = jul,
  pages     = {1126--1135},
  publisher = {{PMLR}},
  issn      = {2640-3498},
  urldate   = {2022-08-05},
  abstract  = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  langid    = {english},
  keywords  = {GFM,read,starred},
  file      = {C\:\\Users\\stane\\Zotero\\storage\\34F8QI5L\\Finn et al. - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation o.pdf;C\:\\Users\\stane\\Zotero\\storage\\VNF2K3NP\\Finn et al (2017) - Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.pdf}
}

@misc{flennerhagMetaLearningWarpedGradient2020,
  title         = {Meta-{{Learning}} with {{Warped Gradient Descent}}},
  author        = {Flennerhag, Sebastian and Rusu, Andrei A. and Pascanu, Razvan and Visin, Francesco and Yin, Hujun and Hadsell, Raia},
  year          = {2020},
  month         = feb,
  number        = {arXiv:1909.00025},
  eprint        = {1909.00025},
  primaryclass  = {cs, stat},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.1909.00025},
  urldate       = {2023-09-11},
  abstract      = {Learning an efficient update rule from data that promotes rapid learning of new tasks from the same distribution remains an open problem in meta-learning. Typically, previous works have approached this issue either by attempting to train a neural network that directly produces updates or by attempting to learn better initialisations or scaling factors for a gradient-based update rule. Both of these approaches pose challenges. On one hand, directly producing an update forgoes a useful inductive bias and can easily lead to non-converging behaviour. On the other hand, approaches that try to control a gradient-based update rule typically resort to computing gradients through the learning process to obtain their meta-gradients, leading to methods that can not scale beyond few-shot task adaptation. In this work, we propose Warped Gradient Descent (WarpGrad), a method that intersects these approaches to mitigate their limitations. WarpGrad meta-learns an efficiently parameterised preconditioning matrix that facilitates gradient descent across the task distribution. Preconditioning arises by interleaving non-linear layers, referred to as warp-layers, between the layers of a task-learner. Warp-layers are meta-learned without backpropagating through the task training process in a manner similar to methods that learn to directly produce updates. WarpGrad is computationally efficient, easy to implement, and can scale to arbitrarily large meta-learning problems. We provide a geometrical interpretation of the approach and evaluate its effectiveness in a variety of settings, including few-shot, standard supervised, continual and reinforcement learning.},
  archiveprefix = {arxiv},
  keywords      = {GFM,skimmed},
  file          = {C:\Users\stane\Zotero\storage\2C4XGWLI\Flennerhag et al (2020) - Meta-Learning with Warped Gradient Descent.pdf}
}

@inproceedings{galantiModularityHypernetworks2020,
  title     = {On the {{Modularity}} of {{Hypernetworks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author    = {Galanti, Tomer and Wolf, Lior},
  year      = {2020},
  volume    = {33},
  pages     = {10409--10419},
  publisher = {{Curran Associates, Inc.}},
  urldate   = {2022-05-09},
  keywords  = {GFM,skimmed},
  file      = {C:\Users\stane\Zotero\storage\7EEP7CCC\Galanti and Wolf - 2020 - On the Modularity of Hypernetworks.pdf}
}

@article{godahewaEnsemblesLocalisedModels2021,
  title    = {Ensembles of Localised Models for Time Series Forecasting},
  author   = {Godahewa, Rakshitha and Bandara, Kasun and Webb, Geoffrey I. and Smyl, Slawek and Bergmeir, Christoph},
  year     = {2021},
  month    = dec,
  journal  = {Knowledge-Based Systems},
  volume   = {233},
  pages    = {107518},
  issn     = {0950-7051},
  doi      = {10.1016/j.knosys.2021.107518},
  urldate  = {2021-11-25},
  abstract = {With large quantities of data typically available nowadays, forecasting models that are trained across sets of time series, known as Global Forecasting Models (GFM), are regularly outperforming traditional univariate forecasting models that work on isolated series. As GFMs usually share the same set of parameters across all time series, they often have the problem of not being localised enough to a particular series, especially in situations where datasets are heterogeneous. We study how ensembling techniques can be used with generic GFMs and univariate models to solve this issue. Our work systematises and compares relevant current approaches, namely clustering series and training separate submodels per cluster, the so-called ensemble of specialists approach, and building heterogeneous ensembles of global and local models. We fill some gaps in the existing GFM localisation approaches, in particular by incorporating varied clustering techniques such as feature-based clustering, distance-based clustering and random clustering, and generalise them to use different underlying GFM model types. We then propose a new methodology of clustered ensembles where we train multiple GFMs on different clusters of series, obtained by changing the number of clusters and cluster seeds. Using Feed-forward Neural Networks, Recurrent Neural Networks, and Pooled Regression models as the underlying GFMs, in our evaluation on eight publicly available datasets, the proposed models are able to achieve significantly higher accuracy than baseline GFM models and univariate forecasting methods.},
  langid   = {english},
  keywords = {GFM,out,read},
  file     = {C:\Users\stane\Zotero\storage\3TX2EWT4\Godahewa et al (2021) - Ensembles of localised models for time series forecasting.pdf}
}

@article{haHyperNetworks2016,
  title         = {{{HyperNetworks}}},
  author        = {Ha, David and Dai, Andrew and Le, Quoc V.},
  year          = {2016},
  month         = dec,
  journal       = {arXiv:1609.09106 [cs]},
  eprint        = {1609.09106},
  primaryclass  = {cs},
  urldate       = {2022-05-09},
  abstract      = {This work explores hypernetworks: an approach of using a one network, also known as a hypernetwork, to generate the weights for another network. Hypernetworks provide an abstraction that is similar to what is found in nature: the relationship between a genotype - the hypernetwork - and a phenotype - the main network. Though they are also reminiscent of HyperNEAT in evolution, our hypernetworks are trained end-to-end with backpropagation and thus are usually faster. The focus of this work is to make hypernetworks useful for deep convolutional networks and long recurrent networks, where hypernetworks can be viewed as relaxed form of weight-sharing across layers. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve near state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks. Our results also show that hypernetworks applied to convolutional networks still achieve respectable results for image recognition tasks compared to state-of-the-art baseline models while requiring fewer learnable parameters.},
  archiveprefix = {arxiv},
  keywords      = {GFM,read,reflection},
  file          = {C\:\\Users\\stane\\Zotero\\storage\\ZKPM9MSY\\Ha et al. - 2016 - HyperNetworks.pdf;C\:\\Users\\stane\\Zotero\\storage\\8M32FC67\\1609.html}
}

@article{hewamalageGlobalModelsTime2021,
  title      = {Global {{Models}} for {{Time Series Forecasting}}: {{A Simulation Study}}},
  shorttitle = {Global {{Models}} for {{Time Series Forecasting}}},
  author     = {Hewamalage, Hansika and Bergmeir, Christoph and Bandara, Kasun},
  year       = {2021},
  month      = nov,
  journal    = {Pattern Recognition},
  pages      = {108441},
  issn       = {0031-3203},
  doi        = {10.1016/j.patcog.2021.108441},
  urldate    = {2021-11-25},
  abstract   = {The recent advances in Big Data have opened up the opportunity to develop competitive Global Forecasting Models (GFM) that simultaneously learn from many time series. Although, the concept of series relatedness has been heavily exploited with GFMs to explain their superiority over local statistical benchmarks, this concept remains largely under-investigated in an empirical setting. Hence, this study attempts to explore the factors that affect GFM performance, by simulating a number of datasets having controllable characteristics. The factors being controlled are along the homogeneity/heterogeneity of series, the complexity of patterns in the series, the complexity of forecasting models, and the lengths/number of series. We simulate time series from simple Data Generating Processes (DGP), such as Auto Regressive (AR), Seasonal AR and Fourier Terms to complex DGPs, such as Chaotic Logistic Map, Self-Exciting Threshold Auto-Regressive and Mackey-Glass Equations. We perform experiments on these datasets using Recurrent Neural Networks (RNN), Feed-Forward Neural Networks, Pooled Regression models and Light Gradient Boosting Models (LGBM) built as GFMs, and compare their performance against standard statistical forecasting techniques. Our experiments demonstrate that with respect to GFM performance, relatedness is closely associated with other factors such as the availability of data, complexity of data and the complexity of the forecasting technique used. Also, techniques such as RNNs and LGBMs having complex non-linear modelling capabilities, when built as GFMs are competitive methods under challenging forecasting scenarios such as short series, heterogeneous series and having minimal prior knowledge of the data patterns.},
  langid     = {english},
  keywords   = {GFM,out,read},
  file       = {C:\Users\stane\Zotero\storage\JRIZ3BSV\Hewamalage et al (2021) - Global Models for Time Series Forecasting.pdf}
}

@article{hewamalageRecurrentNeuralNetworks2021,
  title      = {Recurrent {{Neural Networks}} for {{Time Series Forecasting}}: {{Current}} Status and Future Directions},
  shorttitle = {Recurrent {{Neural Networks}} for {{Time Series Forecasting}}},
  author     = {Hewamalage, Hansika and Bergmeir, Christoph and Bandara, Kasun},
  year       = {2021},
  month      = jan,
  journal    = {International Journal of Forecasting},
  volume     = {37},
  number     = {1},
  pages      = {388--427},
  issn       = {0169-2070},
  doi        = {10.1016/j.ijforecast.2020.06.008},
  urldate    = {2021-12-14},
  abstract   = {Recurrent Neural Networks (RNNs) have become competitive forecasting methods, as most notably shown in the winning method of the recent M4 competition. However, established statistical models such as exponential smoothing (ETS) and the autoregressive integrated moving average (ARIMA) gain their popularity not only from their high accuracy, but also because they are suitable for non-expert users in that they are robust, efficient, and automatic. In these areas, RNNs have still a long way to go. We present an extensive empirical study and an open-source software framework of existing RNN architectures for forecasting, and we develop guidelines and best practices for their use. For example, we conclude that RNNs are capable of modelling seasonality directly if the series in the dataset possess homogeneous seasonal patterns; otherwise, we recommend a deseasonalisation step. Comparisons against ETS and ARIMA demonstrate that (semi-) automatic RNN models are not silver bullets, but they are nevertheless competitive alternatives in many situations.},
  langid     = {english},
  keywords   = {GFM,read},
  file       = {C:\Users\stane\Zotero\storage\AC9MKMYH\Hewamalage et al. - 2021 - Recurrent Neural Networks for Time Series Forecast.pdf}
}

@article{hospedalesMetalearningNeuralNetworks2021,
  title      = {Meta-Learning in Neural Networks: {{A}} Survey},
  shorttitle = {Meta-Learning in Neural Networks},
  author     = {Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
  year       = {2021},
  journal    = {IEEE transactions on pattern analysis and machine intelligence},
  volume     = {44},
  number     = {9},
  pages      = {5149--5169},
  publisher  = {{IEEE}},
  keywords   = {GFM,read,reflection},
  file       = {C:\Users\stane\Zotero\storage\LTJE3GPN\Hospedales et al (2020) - Meta-learning in neural networks.pdf}
}

@article{kimBayesianModelAgnosticMetaLearning2018,
  title         = {Bayesian {{Model-Agnostic Meta-Learning}}},
  author        = {Kim, Taesup and Yoon, Jaesik and Dia, Ousmane and Kim, Sungwoong and Bengio, Yoshua and Ahn, Sungjin},
  year          = {2018},
  month         = nov,
  journal       = {arXiv:1806.03836 [cs, stat]},
  eprint        = {1806.03836},
  primaryclass  = {cs, stat},
  urldate       = {2021-12-31},
  abstract      = {Learning to infer Bayesian posterior from a few-shot dataset is an important step towards robust meta-learning due to the model uncertainty inherent in the problem. In this paper, we propose a novel Bayesian model-agnostic meta-learning method. The proposed method combines scalable gradient-based meta-learning with nonparametric variational inference in a principled probabilistic framework. During fast adaptation, the method is capable of learning complex uncertainty structure beyond a point estimate or a simple Gaussian approximation. In addition, a robust Bayesian meta-update mechanism with a new meta-loss prevents overfitting during meta-update. Remaining an efficient gradient-based meta-learner, the method is also model-agnostic and simple to implement. Experiment results show the accuracy and robustness of the proposed method in various tasks: sinusoidal regression, image classification, active learning, and reinforcement learning.},
  archiveprefix = {arxiv},
  keywords      = {GFM,waiting},
  file          = {C:\Users\stane\Zotero\storage\HDX9UFM3\Kim et al. - 2018 - Bayesian Model-Agnostic Meta-Learning.pdf}
}

@inproceedings{leeGradientBasedMetaLearningLearned2018,
  title     = {Gradient-{{Based Meta-Learning}} with {{Learned Layerwise Metric}} and {{Subspace}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author    = {Lee, Yoonho and Choi, Seungjin},
  year      = {2018},
  month     = jul,
  pages     = {2927--2936},
  publisher = {{PMLR}},
  issn      = {2640-3498},
  urldate   = {2022-07-31},
  abstract  = {Gradient-based meta-learning methods leverage gradient descent to learn the commonalities among various tasks. While previous such methods have been successful in meta-learning tasks, they resort to simple gradient descent during meta-testing. Our primary contribution is the MT-net, which enables the meta-learner to learn on each layer's activation space a subspace that the task-specific learner performs gradient descent on. Additionally, a task-specific learner of an MT-net performs gradient descent with respect to a meta-learned distance metric, which warps the activation space to be more sensitive to task identity. We demonstrate that the dimension of this learned subspace reflects the complexity of the task-specific learner's adaptation task, and also that our model is less sensitive to the choice of initial learning rates than previous gradient-based meta-learning methods. Our method achieves state-of-the-art or comparable performance on few-shot classification and regression tasks.},
  langid    = {english},
  keywords  = {GFM,read,reflection},
  file      = {C\:\\Users\\stane\\Zotero\\storage\\JWCPCNFK\\Lee, Choi (2018) - Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace.pdf;C\:\\Users\\stane\\Zotero\\storage\\Z238ACMY\\Lee and Choi - 2018 - Gradient-Based Meta-Learning with Learned Layerwis.pdf}
}

@misc{liMetaSGDLearningLearn2017,
  title         = {Meta-{{SGD}}: {{Learning}} to {{Learn Quickly}} for {{Few-Shot Learning}}},
  shorttitle    = {Meta-{{SGD}}},
  author        = {Li, Zhenguo and Zhou, Fengwei and Chen, Fei and Li, Hang},
  year          = {2017},
  month         = sep,
  number        = {arXiv:1707.09835},
  eprint        = {1707.09835},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.1707.09835},
  urldate       = {2022-08-12},
  abstract      = {Few-shot learning is challenging for learning algorithms that learn each task in isolation and from scratch. In contrast, meta-learning learns from many related tasks a meta-learner that can learn a new task more accurately and faster with fewer examples, where the choice of meta-learners is crucial. In this paper, we develop Meta-SGD, an SGD-like, easily trainable meta-learner that can initialize and adapt any differentiable learner in just one step, on both supervised learning and reinforcement learning. Compared to the popular meta-learner LSTM, Meta-SGD is conceptually simpler, easier to implement, and can be learned more efficiently. Compared to the latest meta-learner MAML, Meta-SGD has a much higher capacity by learning to learn not just the learner initialization, but also the learner update direction and learning rate, all in a single meta-learning process. Meta-SGD shows highly competitive performance for few-shot learning on regression, classification, and reinforcement learning.},
  archiveprefix = {arxiv},
  keywords      = {GFM,read},
  file          = {C:\Users\stane\Zotero\storage\QCZBRXP9\Li et al (2017) - Meta-SGD.pdf}
}

@article{linLearnEffectiveFeatures2020,
  title      = {To {{Learn Effective Features}}: {{Understanding}} the {{Task-Specific Adaptation}} of {{MAML}}},
  shorttitle = {To {{Learn Effective Features}}},
  author     = {Lin, Zhijie and Zhao, Zhou and Zhang, Zhu and Baoxing, Huai and Yuan, Jing},
  year       = {2020},
  keywords   = {GFM,read},
  file       = {C:\Users\stane\Zotero\storage\ZT7TNKNY\Lin et al (2020) - To Learn Effective Features.pdf}
}

@misc{machinelearningtvLearningLearnIntroduction2020,
  title      = {Learning to Learn: {{An Introduction}} to {{Meta Learning}}},
  shorttitle = {Learning to Learn},
  author     = {{Machine Learning TV}},
  year       = {2020},
  month      = feb,
  urldate    = {2022-01-04},
  abstract   = {Slides PDF: https://drive.google.com/file/d/1DuHy... Abstract: In recent years, high-capacity models, such as deep neural networks, have enabled very powerful machine learning techniques in domains where data is plentiful. However, domains where data is scarce have proven challenging for such methods because high-capacity function approximators critically rely on large datasets for generalization. This can pose a major challenge for domains ranging from supervised medical image processing to reinforcement learning where real-world data collection (e.g., for robots) poses a major logistical challenge. Meta-learning or few-shot learning offers a potential solution to this problem: by learning to learn across data from many previous tasks, few-shot meta-learning algorithms can discover the structure among tasks to enable fast learning of new tasks.  The objective of this tutorial is to provide a unified perspective of meta-learning: teaching the audience about modern approaches, describing the conceptual and theoretical principles surrounding these techniques, presenting where these methods have been applied previously, and discussing the fundamental open problems and challenges within the area. We hope that this tutorial is useful for both machine learning researchers whose expertise lies in other areas, while also providing a new perspective to meta-learning researchers. All in all, we aim to provide audience members with the ability to apply meta-learning to their own applications, and develop new meta-learning algorithms and theoretical analyses driven by the current challenges and limitations of existing work.  We will provide a unified perspective of how a variety of meta-learning algorithms enable learning from small datasets, an overview of applications where meta-learning can and cannot be easily applied, and a discussion of the outstanding challenges and frontiers of this sub-field.},
  keywords   = {GFM,read},
  file       = {C:\Users\stane\Zotero\storage\LS9JXGQN\Machine Learning TV (2020) - Learning to learn.pdf}
}

@article{makridakisM5CompetitionBackground2021,
  title      = {The {{M5}} Competition: {{Background}}, Organization, and Implementation},
  shorttitle = {The {{M5}} Competition},
  author     = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
  year       = {2021},
  month      = sep,
  journal    = {International Journal of Forecasting},
  issn       = {0169-2070},
  doi        = {10.1016/j.ijforecast.2021.07.007},
  urldate    = {2021-11-27},
  abstract   = {The M5 competition follows the previous four M competitions, whose purpose is to learn from empirical evidence how to improve forecasting performance and advance the theory and practice of forecasting. M5 focused on a retail sales forecasting application with the objective to produce the most accurate point forecasts for 42,840 time series that represent the hierarchical unit sales of the largest retail company in the world, Walmart, as well as to provide the most accurate estimates of the uncertainty of these forecasts. Hence, the competition consisted of two parallel challenges, namely the Accuracy and Uncertainty forecasting competitions. M5 extended the results of the previous M competitions by: (a) significantly expanding the number of participating methods, especially those in the category of machine learning; (b) evaluating the performance of the uncertainty distribution along with point forecast accuracy; (c) including exogenous/explanatory variables in addition to the time series data; (d) using grouped, correlated time series; and (e) focusing on series that display intermittency. This paper describes the background, organization, and implementations of the competition, and it presents the data used and their characteristics. Consequently, it serves as introductory material to the results of the two forecasting challenges to facilitate their understanding.},
  langid     = {english},
  keywords   = {GFM,read},
  file       = {C:\Users\stane\Zotero\storage\XUCPXVJU\Makridakis et al (2021) - The M5 competition.pdf}
}

@article{makridakisPredictingHypothesizingFindings2021,
  title    = {Predicting/Hypothesizing the Findings of the {{M5}} Competition},
  author   = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
  year     = {2021},
  month    = nov,
  journal  = {International Journal of Forecasting},
  issn     = {0169-2070},
  doi      = {10.1016/j.ijforecast.2021.09.014},
  urldate  = {2021-11-27},
  abstract = {The scientific method consists of making hypotheses or predictions and then carrying out experiments to test them once the actual results have become available, in order to learn from both successes and mistakes. This approach was followed in the M4 competition with positive results and has been repeated in the M5, with its organizers submitting their ten predictions/hypotheses about its expected results five days before its launch. The present paper presents these predictions/hypotheses and evaluates their realization according to the actual findings of the competition. The results indicate that well-established practices, like combining forecasts, exploiting explanatory variables, and capturing seasonality and special days, remain critical for enhancing forecasting performance, re-confirming also that relatively new approaches, like cross-learning algorithms and machine learning methods, display great potential. Yet, we show that simple, local statistical methods may still be competitive for forecasting high granularity data and estimating the tails of the uncertainty distribution, thus motivating future research in the field of retail sales forecasting.},
  langid   = {english},
  keywords = {GFM,read},
  file     = {C:\Users\stane\Zotero\storage\4QAD7WYM\Makridakis et al (2021) - Predicting-hypothesizing the findings of the M5 competition.pdf}
}

@article{montero-mansoPrinciplesAlgorithmsForecasting2021,
  title      = {Principles and Algorithms for Forecasting Groups of Time Series: {{Locality}} and Globality},
  shorttitle = {Principles and Algorithms for Forecasting Groups of Time Series},
  author     = {{Montero-Manso}, Pablo and Hyndman, Rob J.},
  year       = {2021},
  month      = oct,
  journal    = {International Journal of Forecasting},
  volume     = {37},
  number     = {4},
  pages      = {1632--1653},
  issn       = {0169-2070},
  doi        = {10.1016/j.ijforecast.2021.03.004},
  urldate    = {2021-11-23},
  abstract   = {Global methods that fit a single forecasting method to all time series in a set have recently shown surprising accuracy, even when forecasting large groups of heterogeneous time series. We provide the following contributions that help understand the potential and applicability of global methods and how they relate to traditional local methods that fit a separate forecasting method to each series: {$\bullet$}Global and local methods can produce the same forecasts without any assumptions about similarity of the series in the set.{$\bullet$}The complexity of local methods grows with the size of the set while it remains constant for global methods. This result supports the recent evidence and provides principles for the design of new algorithms.{$\bullet$}In an extensive empirical study, we show that purposely na{\"i}ve algorithms derived from these principles show outstanding accuracy. In particular, global linear models provide competitive accuracy with far fewer parameters than the simplest of local methods.},
  langid     = {english},
  keywords   = {GFM,idea,out,read,starred},
  file       = {C\:\\Users\\stane\\Zotero\\storage\\V47E6AGD\\Montero-Manso and Hyndman - 2021 - Principles and algorithms for forecasting groups o.pdf;C\:\\Users\\stane\\Zotero\\storage\\XRHQJ642\\Appendix.pdf}
}

@misc{navaMetaLearningClassifierFree2023,
  title         = {Meta-{{Learning}} via {{Classifier}}(-Free) {{Diffusion Guidance}}},
  author        = {Nava, Elvis and Kobayashi, Seijin and Yin, Yifei and Katzschmann, Robert K. and Grewe, Benjamin F.},
  year          = {2023},
  month         = jan,
  number        = {arXiv:2210.08942},
  eprint        = {2210.08942},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2210.08942},
  urldate       = {2023-09-11},
  abstract      = {We introduce meta-learning algorithms that perform zero-shot weight-space adaptation of neural network models to unseen tasks. Our methods repurpose the popular generative image synthesis techniques of natural language guidance and diffusion models to generate neural network weights adapted for tasks. We first train an unconditional generative hypernetwork model to produce neural network weights; then we train a second "guidance" model that, given a natural language task description, traverses the hypernetwork latent space to find high-performance task-adapted weights in a zero-shot manner. We explore two alternative approaches for latent space guidance: "HyperCLIP"-based classifier guidance and a conditional Hypernetwork Latent Diffusion Model ("HyperLDM"), which we show to benefit from the classifier-free guidance technique common in image generation. Finally, we demonstrate that our approaches outperform existing multi-task and meta-learning methods in a series of zero-shot learning experiments on our Meta-VQA dataset.},
  archiveprefix = {arxiv},
  keywords      = {GFM,read},
  file          = {C:\Users\stane\Zotero\storage\3UBRXWHY\Nava et al (2023) - Meta-Learning via Classifier(-free) Diffusion Guidance.pdf}
}

@article{parkMetacurvature2019,
  title    = {Meta-Curvature},
  author   = {Park, Eunbyung and Oliva, Junier B.},
  year     = {2019},
  journal  = {Advances in Neural Information Processing Systems},
  volume   = {32},
  keywords = {GFM,skimmed},
  file     = {C:\Users\stane\Zotero\storage\9SHJN5TV\Park, Oliva (2019) - Meta-curvature.pdf}
}

@article{qinDualStageAttentionBasedRecurrent2017,
  title         = {A {{Dual-Stage Attention-Based Recurrent Neural Network}} for {{Time Series Prediction}}},
  author        = {Qin, Yao and Song, Dongjin and Chen, Haifeng and Cheng, Wei and Jiang, Guofei and Cottrell, Garrison},
  year          = {2017},
  month         = aug,
  journal       = {arXiv:1704.02971 [cs, stat]},
  eprint        = {1704.02971},
  primaryclass  = {cs, stat},
  urldate       = {2021-12-14},
  abstract      = {The Nonlinear autoregressive exogenous (NARX) model, which predicts the current value of a time series based upon its previous values as well as the current and past values of multiple driving (exogenous) series, has been studied for decades. Despite the fact that various NARX models have been developed, few of them can capture the long-term temporal dependencies appropriately and select the relevant driving series to make predictions. In this paper, we propose a dual-stage attention-based recurrent neural network (DA-RNN) to address these two issues. In the first stage, we introduce an input attention mechanism to adaptively extract relevant driving series (a.k.a., input features) at each time step by referring to the previous encoder hidden state. In the second stage, we use a temporal attention mechanism to select relevant encoder hidden states across all time steps. With this dual-stage attention scheme, our model can not only make predictions effectively, but can also be easily interpreted. Thorough empirical studies based upon the SML 2010 dataset and the NASDAQ 100 Stock dataset demonstrate that the DA-RNN can outperform state-of-the-art methods for time series prediction.},
  archiveprefix = {arxiv},
  keywords      = {GFM,read},
  file          = {C:\Users\stane\Zotero\storage\Z2QITYT7\Qin et al (2017) - A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction.pdf}
}

@article{raghuRapidLearningFeature2019,
  title         = {Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of Maml},
  shorttitle    = {Rapid Learning or Feature Reuse?},
  author        = {Raghu, Aniruddh and Raghu, Maithra and Bengio, Samy and Vinyals, Oriol},
  year          = {2019},
  journal       = {arXiv preprint arXiv:1909.09157},
  eprint        = {1909.09157},
  archiveprefix = {arxiv},
  keywords      = {GFM,read},
  file          = {C:\Users\stane\Zotero\storage\3B2332UX\Raghu et al (2019) - Rapid learning or feature reuse.pdf}
}

@article{ramanarayananGeneralizingSupervisedDeep2023,
  title    = {Generalizing Supervised Deep Learning {{MRI}} Reconstruction to Multiple and Unseen Contrasts Using Meta-Learning Hypernetworks},
  author   = {Ramanarayanan, Sriprabha and Palla, Arun and Ram, Keerthi and Sivaprakasam, Mohanasankar},
  year     = {2023},
  month    = oct,
  journal  = {Applied Soft Computing},
  volume   = {146},
  pages    = {110633},
  issn     = {1568-4946},
  doi      = {10.1016/j.asoc.2023.110633},
  urldate  = {2023-09-11},
  abstract = {Meta-learning has recently been an emerging data-efficient learning technique for various medical imaging operations and has helped advance contemporary deep learning models. Furthermore, meta-learning enhances the knowledge generalization of the imaging tasks by learning both shared and discriminative weights for various configurations of imaging tasks during training. However, existing meta-learning models attempt to learn a single set of weight initializations of a neural network that might be fundamentally restrictive under the heterogeneous (multimodal) data scenario. This work aims to develop a multimodal meta-learning model for image reconstruction, which augments meta-learning with evolutionary capabilities to encompass diverse acquisition settings of heterogeneous data. Our proposed model called KM-MAML (Kernel Modulation-based Multimodal Meta-Learning), has hypernetworks (auxiliary learners) that evolve to generate mode-specific (or context-specific) weights. These weights provide the mode-specific inductive bias for multiple modes by re-calibrating each kernel of the base network for image reconstruction via a low-rank kernel modulation operation. Furthermore, we incorporate gradient-based meta-learning (GBML) in the contextual space to update the weights of the hypernetworks based on different modes. The hypernetworks and the base reconstruction network in the GBML setting provide discriminative mode-specific features and low-level image features, respectively. We extensively evaluate our model for multi-contrast magnetic resonance image reconstruction considering the essential research directions in fastMRI for multimodal and rich transfer learning capabilities across various MRI contrasts. Our comparative studies show that the proposed model (i) exhibits superior reconstruction performance over joint training, other meta-learning methods, and various context-specific MRI reconstruction architectures, and (ii) better adaptation to 80\% and 92\% of unseen multi-contrast data contexts with improvement margins of 0.1 to 0.5 dB in PSNR and around 0.01 in SSIM, respectively. Besides, a representation analysis with U-Net as the base network shows that kernel modulation infuses 80\% of mode-specific representation changes in the high-resolution layers. Our source code is available at https://github.com/sriprabhar/KM-MAML/.},
  keywords = {GFM,skimmed},
  file     = {C:\Users\stane\Zotero\storage\H74IWJBW\Ramanarayanan et al (2023) - Generalizing supervised deep learning MRI reconstruction to multiple and unseen contrasts using meta-learning hypernetworks.pdf}
}

@article{raskuttiEarlyStoppingNonparametric2014,
  title      = {Early Stopping and Non-Parametric Regression: An Optimal Data-Dependent Stopping Rule},
  shorttitle = {Early Stopping and Non-Parametric Regression},
  author     = {Raskutti, Garvesh and Wainwright, Martin J. and Yu, Bin},
  year       = {2014},
  journal    = {The Journal of Machine Learning Research},
  volume     = {15},
  number     = {1},
  pages      = {335--366},
  publisher  = {{JMLR. org}},
  keywords   = {GFM,skimmed},
  file       = {C:\Users\stane\Zotero\storage\NDTIWU76\Raskutti et al (2014) - Early stopping and non-parametric regression.pdf}
}

@book{ravichandiranHandsOnMetaLearning2018,
  title      = {Hands-{{On Meta Learning}} with {{Python}}: {{Meta}} Learning Using One-Shot Learning, {{MAML}}, {{Reptile}}, and {{Meta-SGD}} with {{TensorFlow}}},
  shorttitle = {Hands-{{On Meta Learning}} with {{Python}}},
  author     = {Ravichandiran, Sudharsan},
  year       = {2018},
  month      = dec,
  publisher  = {{Packt Publishing}},
  address    = {{Birmingham, UK}},
  isbn       = {978-1-78953-420-7},
  langid     = {english},
  keywords   = {GFM,waiting},
  file       = {C:\Users\stane\Zotero\storage\ZQ7UENTE\Ravichandiran - 2018 - Hands-On Meta Learning with Python Meta learning .pdf}
}

@article{requeimaFastFlexibleMultiTask2020,
  title         = {Fast and {{Flexible Multi-Task Classification Using Conditional Neural Adaptive Processes}}},
  author        = {Requeima, James and Gordon, Jonathan and Bronskill, John and Nowozin, Sebastian and Turner, Richard E.},
  year          = {2020},
  month         = jan,
  journal       = {arXiv:1906.07697 [cs, stat]},
  eprint        = {1906.07697},
  primaryclass  = {cs, stat},
  urldate       = {2022-05-09},
  abstract      = {The goal of this paper is to design image classification systems that, after an initial multi-task training phase, can automatically adapt to new tasks encountered at test time. We introduce a conditional neural process based approach to the multi-task classification setting for this purpose, and establish connections to the meta-learning and few-shot learning literature. The resulting approach, called CNAPs, comprises a classifier whose parameters are modulated by an adaptation network that takes the current task's dataset as input. We demonstrate that CNAPs achieves state-of-the-art results on the challenging Meta-Dataset benchmark indicating high-quality transfer-learning. We show that the approach is robust, avoiding both over-fitting in low-shot regimes and under-fitting in high-shot regimes. Timing experiments reveal that CNAPs is computationally efficient at test-time as it does not involve gradient based adaptation. Finally, we show that trained models are immediately deployable to continual learning and active learning where they can outperform existing approaches that do not leverage transfer learning.},
  archiveprefix = {arxiv},
  keywords      = {GFM,read,reflection},
  file          = {C\:\\Users\\stane\\Zotero\\storage\\PNKHMTV4\\Requeima et al. - 2020 - Fast and Flexible Multi-Task Classification Using .pdf;C\:\\Users\\stane\\Zotero\\storage\\BEXLSQNB\\1906.html}
}

@book{sarkarHandsOnTransferLearning2018,
  title      = {Hands-{{On Transfer Learning}} with {{Python}}: {{Implement}} Advanced Deep Learning and Neural Network Models Using {{TensorFlow}} and {{Keras}}},
  shorttitle = {Hands-{{On Transfer Learning}} with {{Python}}},
  author     = {Sarkar, Dipanjan and Bali, Raghav and Ghosh, Tamoghna},
  year       = {2018},
  month      = aug,
  edition    = {1st edition},
  publisher  = {{Packt Publishing}},
  langid     = {english},
  keywords   = {GFM,waiting},
  file       = {C:\Users\stane\Zotero\storage\QTHRYM5S\Dipanjan Sarkar, Raghav Bali, Tamoghna Ghosh - Hands-On Transfer Learning with Python Implement Advanced Deep Learning and Neural Network Models Using TensorFlow and Keras-Packt Publishing (2018).epub}
}

@inproceedings{shamsianPersonalizedFederatedLearning2021,
  title     = {Personalized {{Federated Learning}} Using {{Hypernetworks}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author    = {Shamsian, Aviv and Navon, Aviv and Fetaya, Ethan and Chechik, Gal},
  year      = {2021},
  month     = jul,
  pages     = {9489--9502},
  publisher = {{PMLR}},
  issn      = {2640-3498},
  urldate   = {2022-05-09},
  abstract  = {Personalized federated learning is tasked with training machine learning models for multiple clients, each with its own data distribution. The goal is to train personalized models collaboratively while accounting for data disparities across clients and reducing communication costs. We propose a novel approach to this problem using hypernetworks, termed pFedHN for personalized Federated HyperNetworks. In this approach, a central hypernetwork model is trained to generate a set of models, one model for each client. This architecture provides effective parameter sharing across clients while maintaining the capacity to generate unique and diverse personal models. Furthermore, since hypernetwork parameters are never transmitted, this approach decouples the communication cost from the trainable model size. We test pFedHN empirically in several personalized federated learning challenges and find that it outperforms previous methods. Finally, since hypernetworks share information across clients, we show that pFedHN can generalize better to new clients whose distributions differ from any client observed during training.},
  langid    = {english},
  keywords  = {GFM,read,reflection,starred},
  file      = {C:\Users\stane\Zotero\storage\RLZXD85K\Shamsian et al. - 2021 - Personalized Federated Learning using Hypernetwork.pdf}
}

@article{vanschorenMetaLearningSurvey2018,
  title         = {Meta-{{Learning}}: {{A Survey}}},
  shorttitle    = {Meta-{{Learning}}},
  author        = {Vanschoren, Joaquin},
  year          = {2018},
  month         = oct,
  journal       = {arXiv:1810.03548 [cs, stat]},
  eprint        = {1810.03548},
  primaryclass  = {cs, stat},
  urldate       = {2022-04-15},
  abstract      = {Meta-learning, or learning to learn, is the science of systematically observing how different machine learning approaches perform on a wide range of learning tasks, and then learning from this experience, or meta-data, to learn new tasks much faster than otherwise possible. Not only does this dramatically speed up and improve the design of machine learning pipelines or neural architectures, it also allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way. In this chapter, we provide an overview of the state of the art in this fascinating and continuously evolving field.},
  archiveprefix = {arxiv},
  keywords      = {GFM,read,reflection},
  file          = {C:\Users\stane\Zotero\storage\N3FHSU36\Vanschoren (2018) - Meta-Learning.pdf}
}

@misc{vonoswaldContinualLearningHypernetworks2022,
  title         = {Continual Learning with Hypernetworks},
  author        = {{von Oswald}, Johannes and Henning, Christian and Grewe, Benjamin F. and Sacramento, Jo{\~a}o},
  year          = {2022},
  month         = apr,
  number        = {arXiv:1906.00695},
  eprint        = {1906.00695},
  primaryclass  = {cs, stat},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.1906.00695},
  urldate       = {2023-09-11},
  abstract      = {Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, we present a novel approach based on task-conditioned hypernetworks, i.e., networks that generate the weights of a target model based on task identity. Continual learning (CL) is less difficult for this class of models thanks to a simple key feature: instead of recalling the input-output relations of all previously seen data, task-conditioned hypernetworks only require rehearsing task-specific weight realizations, which can be maintained in memory using a simple regularizer. Besides achieving state-of-the-art performance on standard CL benchmarks, additional experiments on long task sequences reveal that task-conditioned hypernetworks display a very large capacity to retain previous memories. Notably, such long memory lifetimes are achieved in a compressive regime, when the number of trainable hypernetwork weights is comparable or smaller than target network size. We provide insight into the structure of low-dimensional task embedding spaces (the input space of the hypernetwork) and show that task-conditioned hypernetworks demonstrate transfer learning. Finally, forward information transfer is further supported by empirical results on a challenging CL benchmark based on the CIFAR-10/100 image datasets.},
  archiveprefix = {arxiv},
  keywords      = {GFM,skimmed},
  file          = {C:\Users\stane\Zotero\storage\YQU7S8HF\von Oswald et al (2022) - Continual learning with hypernetworks.pdf}
}

@inproceedings{wangBridgingMultitaskLearning2021,
  title      = {Bridging Multi-Task Learning and Meta-Learning: {{Towards}} Efficient Training and Effective Adaptation},
  shorttitle = {Bridging Multi-Task Learning and Meta-Learning},
  booktitle  = {International {{Conference}} on {{Machine Learning}}},
  author     = {Wang, Haoxiang and Zhao, Han and Li, Bo},
  year       = {2021},
  pages      = {10991--11002},
  publisher  = {{PMLR}},
  keywords   = {GFM,read,reflection},
  file       = {C:\Users\stane\Zotero\storage\VYNCKFUM\Wang et al (2021) - Bridging multi-task learning and meta-learning.pdf}
}

@article{wangMetalearningNaturalArtificial2021,
  title    = {Meta-Learning in Natural and Artificial Intelligence},
  author   = {Wang, Jane X},
  year     = {2021},
  month    = apr,
  journal  = {Current Opinion in Behavioral Sciences},
  series   = {Computational Cognitive Neuroscience},
  volume   = {38},
  pages    = {90--95},
  issn     = {2352-1546},
  doi      = {10.1016/j.cobeha.2021.01.002},
  urldate  = {2022-04-15},
  abstract = {Meta-learning, or learning to learn, has gained renewed interest in recent years within the artificial intelligence community. However, meta-learning is incredibly prevalent within nature, has deep roots in cognitive science and psychology, and is currently studied in various forms within neuroscience. The aim of this review is to recast previous lines of research in the study of biological intelligence within the lens of meta-learning, placing these works into a common framework. More recent points of interaction between AI and neuroscience will be discussed, as well as interesting new directions that arise under this perspective.},
  langid   = {english},
  keywords = {GFM,read},
  file     = {C:\Users\stane\Zotero\storage\IQQJDTFY\Wang (2021) - Meta-learning in natural and artificial intelligence.pdf}
}

@article{xianHyperDynamicsMetaLearningObject2021,
  title         = {{{HyperDynamics}}: {{Meta-Learning Object}} and {{Agent Dynamics}} with {{Hypernetworks}}},
  shorttitle    = {{{HyperDynamics}}},
  author        = {Xian, Zhou and Lal, Shamit and Tung, Hsiao-Yu and Platanios, Emmanouil Antonios and Fragkiadaki, Katerina},
  year          = {2021},
  month         = mar,
  journal       = {arXiv:2103.09439 [cs]},
  eprint        = {2103.09439},
  primaryclass  = {cs},
  urldate       = {2022-05-09},
  abstract      = {We propose HyperDynamics, a dynamics meta-learning framework that conditions on an agent's interactions with the environment and optionally its visual observations, and generates the parameters of neural dynamics models based on inferred properties of the dynamical system. Physical and visual properties of the environment that are not part of the low-dimensional state yet affect its temporal dynamics are inferred from the interaction history and visual observations, and are implicitly captured in the generated parameters. We test HyperDynamics on a set of object pushing and locomotion tasks. It outperforms existing dynamics models in the literature that adapt to environment variations by learning dynamics over high dimensional visual observations, capturing the interactions of the agent in recurrent state representations, or using gradient-based meta-optimization. We also show our method matches the performance of an ensemble of separately trained experts, while also being able to generalize well to unseen environment variations at test time. We attribute its good performance to the multiplicative interactions between the inferred system properties -- captured in the generated parameters -- and the low-dimensional state representation of the dynamical system.},
  archiveprefix = {arxiv},
  keywords      = {GFM,read,reflection},
  file          = {C\:\\Users\\stane\\Zotero\\storage\\6B4IELF3\\Xian et al. - 2021 - HyperDynamics Meta-Learning Object and Agent Dyna.pdf;C\:\\Users\\stane\\Zotero\\storage\\G427ZZB9\\2103.html}
}

@article{yangDeepMultitaskRepresentation2016,
  title         = {Deep Multi-Task Representation Learning: {{A}} Tensor Factorisation Approach},
  shorttitle    = {Deep Multi-Task Representation Learning},
  author        = {Yang, Yongxin and Hospedales, Timothy},
  year          = {2016},
  journal       = {arXiv preprint arXiv:1605.06391},
  eprint        = {1605.06391},
  archiveprefix = {arxiv},
  keywords      = {GFM,read},
  file          = {C:\Users\stane\Zotero\storage\TWNUQ2YZ\Yang, Hospedales (2016) - Deep multi-task representation learning.pdf}
}

@article{yaoEarlyStoppingGradient2007,
  title     = {On Early Stopping in Gradient Descent Learning},
  author    = {Yao, Yuan and Rosasco, Lorenzo and Caponnetto, Andrea},
  year      = {2007},
  journal   = {Constructive Approximation},
  volume    = {26},
  number    = {2},
  pages     = {289--315},
  publisher = {{Springer}},
  keywords  = {GFM,skimmed},
  file      = {C:\Users\stane\Zotero\storage\W9EMHWWZ\Yao et al (2007) - On early stopping in gradient descent learning.pdf}
}

@inproceedings{zhaoMetaLearningHypernetworks2020,
  title     = {Meta-{{Learning}} via {{Hypernetworks}}},
  booktitle = {Zhao, {{Dominic}}; {{Kobayashi}}, {{Seijin}}; {{Sacramento}}, {{Jo{\~a}o}}; von {{Oswald}}, {{Johannes}}  (2020). {{Meta-Learning}} via {{Hypernetworks}}.  {{In}}: 4th {{Workshop}} on {{Meta-Learning}} at {{NeurIPS}} 2020 ({{MetaLearn}} 2020), {{Virtual Conference}}, 11 {{December}} 2020, {{IEEE}}.},
  author    = {Zhao, Dominic and Kobayashi, Seijin and Sacramento, Jo{\~a}o and {von Oswald}, Johannes},
  year      = {2020},
  month     = dec,
  publisher = {{IEEE}},
  address   = {{Virtual Conference}},
  doi       = {10.5167/uzh-200298},
  urldate   = {2022-05-09},
  abstract  = {Recent developments in few-shot learning have shown that during fast adaption, gradient-based meta-learners mostly rely on embedding features of powerful pretrained networks. This leads us to research ways to effectively adapt features and utilize the meta-learner's full potential. Here, we demonstrate the effectiveness of hypernetworks in this context. We propose a soft row-sharing hypernetwork architecture and show that training the hypernetwork with a variant of MAML is tightly linked to meta-learning a curvature matrix used to condition gradients during fast adaptation. We achieve similar results as state-of-art model-agnostic methods in the overparametrized case, while outperforming many MAML variants without using different optimization schemes in the compressive regime. Furthermore, we empirically show that hypernetworks do leverage the inner loop optimization for better adaptation, and analyse how they naturally try to learn the shared curvature of constructed tasks on a toy problem when using our proposed training algorithm.},
  copyright = {info:eu-repo/semantics/openAccess},
  langid    = {english},
  keywords  = {GFM,read,reflection},
  file      = {C\:\\Users\\stane\\Zotero\\storage\\668KQLQV\\Zhao et al. - 2020 - Meta-Learning via Hypernetworks.pdf;C\:\\Users\\stane\\Zotero\\storage\\W8N9MYVX\\200298.html}
}

@inproceedings{zintgrafFastContextAdaptation2019,
  title     = {Fast Context Adaptation via Meta-Learning},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author    = {Zintgraf, Luisa and Shiarli, Kyriacos and Kurin, Vitaly and Hofmann, Katja and Whiteson, Shimon},
  year      = {2019},
  pages     = {7693--7702},
  publisher = {{PMLR}},
  keywords  = {GFM,read},
  file      = {C:\Users\stane\Zotero\storage\5J8ZXUHG\Zintgraf et al (2019) - Fast context adaptation via meta-learning.pdf}
}


@article{andererHierarchicalForecastingTopdown2022,
  title = {Hierarchical Forecasting with a Top-down Alignment of Independent-Level Forecasts},
  author = {Anderer, Matthias and Li, Feng},
  year = {2022},
  month = oct,
  journal = {International Journal of Forecasting},
  series = {Special {{Issue}}: {{M5}} Competition},
  volume = {38},
  number = {4},
  pages = {1405--1414},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2021.12.015},
  urldate = {2023-07-20},
  abstract = {Hierarchical forecasting with intermittent time series is a challenge in both research and empirical studies. Extensive research focuses on improving the accuracy of each hierarchy, especially the intermittent time series at bottom levels. Then, hierarchical reconciliation can be used to improve the overall performance further. In this paper, we present a hierarchical-forecasting-with-alignment approach that treats the bottom-level forecasts as mutable to ensure higher forecasting accuracy on the upper levels of the hierarchy. We employ a pure deep learning forecasting approach, N-BEATS, for continuous time series at the top levels, and a widely used tree-based algorithm, LightGBM, for intermittent time series at the bottom level. The hierarchical-forecasting-with-alignment approach is a simple yet effective variant of the bottom-up method, accounting for biases that are difficult to observe at the bottom level. It allows suboptimal forecasts at the lower level to retain a higher overall performance. The approach in this empirical study was developed by the first author during the M5 Accuracy competition, ranking second place. The method is also business orientated and can be used to facilitate strategic business planning.},
  langid = {english},
  keywords = {M6,skimmed},
  file = {C:\Users\stane\Zotero\storage\NCKBFHFN\Anderer and Li - 2022 - Hierarchical forecasting with a top-down alignment.pdf}
}

@article{bandaraFastScalableEnsemble2022,
  title = {A Fast and Scalable Ensemble of Global Models with Long Memory and Data Partitioning for the {{M5}} Forecasting Competition},
  author = {Bandara, Kasun and Hewamalage, Hansika and Godahewa, Rakshitha and Gamakumara, Puwasala},
  year = {2022},
  month = oct,
  journal = {International Journal of Forecasting},
  series = {Special {{Issue}}: {{M5}} Competition},
  volume = {38},
  number = {4},
  pages = {1400--1404},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2021.11.004},
  urldate = {2023-07-20},
  abstract = {This work presents key insights on the model development strategies used in our cross-learning-based retail demand forecast framework. The proposed framework outperforms state-of-the-art univariate models in the time series forecasting literature. It has achieved 17th position in the accuracy track of the M5 forecasting competition, which is among the top 1\% of solutions.},
  langid = {english},
  keywords = {M6,skimmed},
  file = {C:\Users\stane\Zotero\storage\FKDNM8AU\Bandara et al (2022) - A fast and scalable ensemble of global models with long memory and data partitioning for the M5 forecasting competition.pdf}
}

@article{bandaraForecastingTimeSeries2020,
  ids = {bandaraForecastingTimeSeries2020a},
  title = {Forecasting across Time Series Databases Using Recurrent Neural Networks on Groups of Similar Series: {{A}} Clustering Approach},
  shorttitle = {Forecasting across Time Series Databases Using Recurrent Neural Networks on Groups of Similar Series},
  author = {Bandara, Kasun and Bergmeir, Christoph and Smyl, Slawek},
  year = {2020},
  month = feb,
  journal = {Expert Systems with Applications},
  volume = {140},
  pages = {112896},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2019.112896},
  urldate = {2020-12-30},
  abstract = {With the advent of Big Data, nowadays in many applications databases containing large quantities of similar time series are available. Forecasting time series in these domains with traditional univariate forecasting procedures leaves great potentials for producing accurate forecasts untapped. Recurrent neural networks (RNNs), and in particular Long Short Term Memory (LSTM) networks, have proven recently that they are able to outperform state-of-the-art univariate time series forecasting methods in this context, when trained across all available time series. However, if the time series database is heterogeneous, accuracy may degenerate, so that on the way towards fully automatic forecasting methods in this space, a notion of similarity between the time series needs to be built into the methods. To this end, we present a prediction model that can be used with different types of RNN models on subgroups of similar time series, which are identified by time series clustering techniques. We assess our proposed methodology using LSTM networks, a widely popular RNN variant, together with various clustering algorithms, such as kMeans, DBScan, Partition Around Medoids (PAM), and Snob. Our method achieves competitive results on benchmarking datasets under competition evaluation procedures. In particular, in terms of mean sMAPE accuracy it consistently outperforms the baseline LSTM model, and outperforms all other methods on the CIF2016 forecasting competition dataset.},
  langid = {english},
  keywords = {M6,out,skimmed},
  file = {C\:\\Users\\stane\\Zotero\\storage\\HSSKDKRD\\Bandara et al. - 2020 - Forecasting across time series databases using rec.pdf;C\:\\Users\\stane\\Zotero\\storage\\R2WJ8MT5\\Bandara et al (2020) - Forecasting across time series databases using recurrent neural networks on groups of similar series.pdf;C\:\\Users\\stane\\Zotero\\storage\\EIEWBD38\\S0957417419306128.html;C\:\\Users\\stane\\Zotero\\storage\\JST4MDZF\\S0957417419306128.html}
}

@misc{chenXgboostExtremeGradient2023,
  title = {Xgboost: {{Extreme Gradient Boosting}}},
  shorttitle = {Xgboost},
  author = {Chen, Tianqi and He, Tong and Benesty, Michael and Khotilovich, Vadim and Tang, Yuan and Cho, Hyunsu and Chen, Kailong and Mitchell, Rory and Cano, Ignacio and Zhou, Tianyi and Li, Mu and Xie, Junyuan and Lin, Min and Geng, Yifeng and Li, Yutian and Yuan, Jiaming and {implementation)}, XGBoost contributors (base XGBoost},
  year = {2023},
  month = mar,
  urldate = {2023-09-20},
  abstract = {Extreme Gradient Boosting, which is an efficient implementation of the gradient boosting framework from Chen \& Guestrin (2016) {$<$}doi:10.1145/2939672.2939785{$>$}. This package is its R interface. The package includes efficient linear model solver and tree learning algorithms. The package can automatically do parallel computation on a single machine which could be more than 10 times faster than existing gradient boosting packages. It supports various objective functions, including regression, classification and ranking. The package is made to be extensible, so that users are also allowed to define their own objectives easily.},
  copyright = {Apache License (== 2.0) {\textbar} file LICENSE},
  keywords = {M6,reference}
}

@incollection{dieboldModelingVolatilityDynamics1995,
  title = {Modeling {{Volatility Dynamics}}},
  booktitle = {Macroeconometrics: {{Developments}}, {{Tensions}}, and {{Prospects}}},
  author = {Diebold, Francis X. and Lopez, Jose A.},
  editor = {Hoover, Kevin D.},
  year = {1995},
  series = {Recent {{Economic Thought Series}}},
  pages = {427--472},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-011-0669-6_11},
  urldate = {2023-06-23},
  abstract = {Good macroeconomic and financial theorists, like all good theorists, want to get the facts straight before theorizing; hence, the explosive growth in the methodology and application of time-series econometrics in the last twenty-five years. Many factors fueled that growth, ranging from important developments in related fields (see Box and Jenkins, 1970) to dissatisfaction with the ``incredible identifying restrictions'' associated with traditional macroeconometric models (Sims, 1980) and the associated recognition that many tasks of interest, such as forecasting, simply do not require a structural model (see Granger and Newbold, 1979). A short list of active subfields includes vector autoregressions, index and dynamic factor models, causality, integration and persistence, cointegration, seasonality, unobserved-components models, state-space representations and the Kalman filter, regime-switching models, nonlinear dynamics, and optimal nonlinear filtering. Any such list must also include models of volatility dynamics. Models of autoregressive conditional heteroskedasticity (ARCH), in particular, provide parsimonious approximations to volatility dynamics and have found wide use in macroeconomics and finance1. The family of ARCH models is the subject of this chapter.},
  isbn = {978-94-011-0669-6},
  langid = {english},
  keywords = {M6,reference},
  file = {C:\Users\stane\Zotero\storage\X3JJN6XE\Diebold, Lopez (1995) - Modeling Volatility Dynamics.pdf}
}

@article{godahewaSETARTreeNovelAccurate2023,
  title = {{{SETAR-Tree}}: A Novel and Accurate Tree Algorithm for Global Time Series Forecasting},
  shorttitle = {{{SETAR-Tree}}},
  author = {Godahewa, Rakshitha and Webb, Geoffrey I. and Schmidt, Daniel and Bergmeir, Christoph},
  year = {2023},
  month = jul,
  journal = {Machine Learning},
  volume = {112},
  number = {7},
  pages = {2555--2591},
  issn = {1573-0565},
  doi = {10.1007/s10994-023-06316-x},
  urldate = {2024-01-22},
  abstract = {Threshold Autoregressive (TAR) models have been widely used by statisticians for non-linear time series forecasting during the past few decades, due to their simplicity and mathematical properties. On the other hand, in the forecasting community, general-purpose tree-based regression algorithms (forests, gradient-boosting) have become popular recently due to their ease of use and accuracy. In this paper, we explore the close connections between TAR models and regression trees. These enable us to use the rich methodology from the literature on TAR models to define a hierarchical TAR model as a regression tree that trains globally across series, which we call SETAR-Tree. In contrast to the general-purpose tree-based models that do not primarily focus on forecasting, and calculate averages at the leaf nodes, we introduce a new forecasting-specific tree algorithm that trains global Pooled Regression (PR) models in the leaves allowing the models to learn cross-series information and also uses some time-series-specific splitting and stopping procedures. The depth of the tree is controlled by conducting a statistical linearity test commonly employed in TAR models, as well as measuring the error reduction percentage at each node split. Thus, the proposed tree model requires minimal external hyperparameter tuning and provides competitive results under its default configuration. We also use this tree algorithm to develop a forest where the forecasts provided by a collection of diverse SETAR-Trees are combined during the forecasting process. In our evaluation on eight publicly available datasets, the proposed tree and forest models are able to achieve significantly higher accuracy than a set of state-of-the-art tree-based algorithms and forecasting benchmarks across four evaluation metrics.},
  langid = {english},
  keywords = {M6,out,waiting},
  file = {C:\Users\stane\Zotero\storage\2NJ7Q5VH\Godahewa et al (2023) - SETAR-Tree.pdf}
}

@misc{hubingerRisksLearnedOptimization2021,
  title = {Risks from {{Learned Optimization}} in {{Advanced Machine Learning Systems}}},
  author = {Hubinger, Evan and {van Merwijk}, Chris and Mikulik, Vladimir and Skalse, Joar and Garrabrant, Scott},
  year = {2021},
  month = dec,
  number = {arXiv:1906.01820},
  eprint = {1906.01820},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2024-02-11},
  abstract = {We analyze the type of learned optimization that occurs when a learned model (such as a neural network) is itself an optimizer - a situation we refer to as mesa-optimization, a neologism we introduce in this paper. We believe that the possibility of mesa-optimization raises two important questions for the safety and transparency of advanced machine learning systems. First, under what circumstances will learned models be optimizers, including when they should not be? Second, when a learned model is an optimizer, what will its objective be - how will it differ from the loss function it was trained under - and how can it be aligned? In this paper, we provide an in-depth analysis of these two primary questions and provide an overview of topics for future research.},
  archiveprefix = {arxiv},
  keywords = {M6,reference},
  file = {C:\Users\stane\Zotero\storage\PNKIUZ8Q\Hubinger et al (2021) - Risks from Learned Optimization in Advanced Machine Learning Systems.pdf}
}

@article{huismanSurveyDeepMetalearning2021,
  title = {A Survey of Deep Meta-Learning},
  author = {Huisman, Mike and {van Rijn}, Jan N. and Plaat, Aske},
  year = {2021},
  month = aug,
  journal = {Artificial Intelligence Review},
  volume = {54},
  number = {6},
  pages = {4483--4541},
  issn = {1573-7462},
  doi = {10.1007/s10462-021-10004-4},
  urldate = {2022-04-15},
  abstract = {Deep neural networks can achieve great successes when presented with large data sets and sufficient computational resources. However, their ability to learn new concepts quickly is limited. Meta-learning is one approach to address this issue, by enabling the network to learn how to learn. The field of Deep Meta-Learning advances at great speed, but lacks a unified, in-depth overview of current techniques. With this work, we aim to bridge this gap. After providing the reader with a theoretical foundation, we investigate and summarize key methods, which are categorized into (i)~metric-, (ii)~model-, and (iii)~optimization-based techniques. In addition, we identify the main open challenges, such as performance evaluations on heterogeneous benchmarks, and reduction of the computational costs of meta-learning.},
  langid = {english},
  keywords = {M6,read,reflection},
  file = {C:\Users\stane\Zotero\storage\FHJY69P8\Huisman et al (2021) - A survey of deep meta-learning.pdf}
}

@article{inSimpleAveragingDirect2022,
  title = {Simple Averaging of Direct and Recursive Forecasts via Partial Pooling Using Machine Learning},
  author = {In, YeonJun and Jung, Jae-Yoon},
  year = {2022},
  month = oct,
  journal = {International Journal of Forecasting},
  series = {Special {{Issue}}: {{M5}} Competition},
  volume = {38},
  number = {4},
  pages = {1386--1399},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2021.11.007},
  urldate = {2023-07-20},
  abstract = {This article introduces the winning method at the M5 Accuracy competition. The presented method takes a simple manner of averaging the results of multiple base forecasting models that have been constructed via partial pooling of multi-level data. All base forecasting models of adopting direct or recursive multi-step forecasting methods are trained by the machine learning technique, LightGBM, from three different levels of data pools. At the competition, the simple averaging of the multiple direct and recursive forecasting models, called DRFAM, obtained the complementary effects between direct and recursive multi-step forecasting of the multi-level product sales to improve the accuracy and the robustness.},
  langid = {english},
  keywords = {M6,skimmed},
  file = {C:\Users\stane\Zotero\storage\7J3SJ65H\In, Jung (2022) - Simple averaging of direct and recursive forecasts via partial pooling using machine learning.pdf}
}

@misc{internationalinstituteofforecastersM6CompetitionISF2023,
  title = {M6 {{Competition}}, {{ISF}} 2023 {{Practitioner Talk}}},
  author = {{International Institute of Forecasters}},
  year = {2023},
  urldate = {2023-08-16},
  abstract = {Fotios Petropoulos, Professor University of Bath, University of Nicosia Evangelos Spiliotis, Research Fellow National Technical University of Athens Presentation: The M6 competition: Key findings and lessons learned},
  keywords = {M6,read}
}

@article{jaganathanCombinationbasedForecastingMethod2020,
  title = {A Combination-Based Forecasting Method for the {{M4-competition}}},
  author = {Jaganathan, Srihari and Prakash, P. K. S.},
  year = {2020},
  month = jan,
  journal = {International Journal of Forecasting},
  series = {M4 {{Competition}}},
  volume = {36},
  number = {1},
  pages = {98--104},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2019.03.030},
  urldate = {2023-07-20},
  abstract = {Several researchers (Armstrong, 2001; Clemen, 1989; Makridakis and Winkler, 1983) have shown empirically that combination-based forecasting methods are very effective in real world settings. This paper discusses a combination-based forecasting approach that was used successfully in the M4 competition. The proposed approach was evaluated on a set of 100K time series across multiple domain areas with varied frequencies. The point forecasts submitted finished fourth based on the overall weighted average (OWA) error measure and second based on the symmetric mean absolute percent error (sMAPE).},
  langid = {english},
  keywords = {M6,skimmed},
  file = {C:\Users\stane\Zotero\storage\2KPG5WJ5\Jaganathan, Prakash (2020) - A combination-based forecasting method for the M4-competition.pdf}
}

@article{januschowskiCriteriaClassifyingForecasting2020,
  title = {Criteria for Classifying Forecasting Methods},
  author = {Januschowski, Tim and Gasthaus, Jan and Wang, Yuyang and Salinas, David and Flunkert, Valentin and {Bohlke-Schneider}, Michael and Callot, Laurent},
  year = {2020},
  month = jan,
  journal = {International Journal of Forecasting},
  series = {M4 {{Competition}}},
  volume = {36},
  number = {1},
  pages = {167--177},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2019.05.008},
  urldate = {2024-01-22},
  abstract = {Classifying forecasting methods as being either of a ``machine learning'' or ``statistical'' nature has become commonplace in parts of the forecasting literature and community, as exemplified by the M4 competition and the conclusion drawn by the organizers. We argue that this distinction does not stem from fundamental differences in the methods assigned to either class. Instead, this distinction is probably of a tribal nature, which limits the insights into the appropriateness and effectiveness of different forecasting methods. We provide alternative characteristics of forecasting methods which, in our view, allow to draw meaningful conclusions. Further, we discuss areas of forecasting which could benefit most from cross-pollination between the ML and the statistics communities.},
  keywords = {M6,out,waiting},
  file = {C:\Users\stane\Zotero\storage\EKC3L834\Januschowski et al (2020) - Criteria for classifying forecasting methods.pdf}
}

@article{kourtisSharpeRatioEstimated2016,
  title = {The {{Sharpe}} Ratio of Estimated Efficient Portfolios},
  author = {Kourtis, Apostolos},
  year = {2016},
  month = may,
  journal = {Finance Research Letters},
  volume = {17},
  pages = {72--78},
  issn = {1544-6123},
  doi = {10.1016/j.frl.2016.01.009},
  urldate = {2023-06-23},
  abstract = {Investors often adopt mean{\textendash}variance efficient portfolios for achieving superior risk-adjusted returns. However, such portfolios are sensitive to estimation errors, which affect portfolio performance. To understand the impact of estimation errors, I develop simple and intuitive formulas of the squared Sharpe ratio that investors should expect from estimated efficient portfolios. The new formulas show that the expected squared Sharpe ratio is a function of the length of the available data, the number of assets and the maximum attainable Sharpe ratio. My results enable the portfolio manager to assess the value of efficient portfolios as investment vehicles, given the investment environment.},
  langid = {english},
  keywords = {M6,reference},
  file = {C:\Users\stane\Zotero\storage\C7P5X434\Kourtis (2016) - The Sharpe ratio of estimated efficient portfolios.pdf}
}

@article{ledoitRobustPerformanceHypothesis2008,
  title = {Robust Performance Hypothesis Testing with the {{Sharpe}} Ratio},
  author = {Ledoit, Oliver and Wolf, Michael},
  year = {2008},
  month = dec,
  journal = {Journal of Empirical Finance},
  volume = {15},
  number = {5},
  pages = {850--859},
  issn = {0927-5398},
  doi = {10.1016/j.jempfin.2008.03.002},
  urldate = {2023-12-05},
  abstract = {Applied researchers often test for the difference of the Sharpe ratios of two investment strategies. A very popular tool to this end is the test of Jobson and Korkie [Jobson, J.D. and Korkie, B.M. (1981). Performance hypothesis testing with the Sharpe and Treynor measures. Journal of Finance, 36:889{\textendash}908], which has been corrected by Memmel [Memmel, C. (2003). Performance hypothesis testing with the Sharpe ratio. Finance Letters, 1:21{\textendash}23]. Unfortunately, this test is not valid when returns have tails heavier than the normal distribution or are of time series nature. Instead, we propose the use of robust inference methods. In particular, we suggest to construct a studentized time series bootstrap confidence interval for the difference of the Sharpe ratios and to declare the two ratios different if zero is not contained in the obtained interval. This approach has the advantage that one can simply resample from the observed data as opposed to some null-restricted data. A simulation study demonstrates the improved finite sample performance compared to existing methods. In addition, two applications to real data are provided.},
  keywords = {M6,waiting},
  file = {C\:\\Users\\stane\\Zotero\\storage\\8TY5NCV2\\Ledoit and Wolf - 2008 - Robust performance hypothesis testing with the Sha.pdf;C\:\\Users\\stane\\Zotero\\storage\\QIQNQ4KQ\\S0927539808000182.html}
}

@misc{makridakisM6FinancialDuathlon2022,
  title = {The {{M6 Financial Duathlon Competition Guidelines}}},
  author = {Makridakis, Spyros and Gaba, A and Hollyman, R and Petropoulos, F and Spiliotis, E and Swanson, N},
  year = {2022},
  keywords = {M6,read}
}

@misc{makridakisM6ForecastingCompetition2023,
  title = {The {{M6}} Forecasting Competition: {{Bridging}} the Gap between Forecasting and Investment Decisions},
  shorttitle = {The {{M6}} Forecasting Competition},
  author = {Makridakis, Spyros and Spiliotis, Evangelos and Hollyman, Ross and Petropoulos, Fotios and Swanson, Norman and Gaba, Anil},
  year = {2023},
  month = oct,
  number = {arXiv:2310.13357},
  eprint = {2310.13357},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.13357},
  urldate = {2023-10-27},
  abstract = {The M6 forecasting competition, the sixth in the Makridakis' competition sequence, is focused on financial forecasting. A key objective of the M6 competition was to contribute to the debate surrounding the Efficient Market Hypothesis (EMH) by examining how and why market participants make investment decisions. To address these objectives, the M6 competition investigated forecasting accuracy and investment performance on a universe of 100 publicly traded assets. The competition employed live evaluation on real data across multiple periods, a cross-sectional setting where participants predicted asset performance relative to that of other assets, and a direct evaluation of the utility of forecasts. In this way, we were able to measure the benefits of accurate forecasting and assess the importance of forecasting when making investment decisions. Our findings highlight the challenges that participants faced when attempting to accurately forecast the relative performance of assets, the great difficulty associated with trying to consistently outperform the market, the limited connection between submitted forecasts and investment decisions, the value added by information exchange and the "wisdom of crowds", and the value of utilizing risk models when attempting to connect prediction and investing decisions.},
  archiveprefix = {arxiv},
  keywords = {M6,read,reflection},
  file = {C:\Users\stane\Zotero\storage\JIHMQBWV\Makridakis et al (2023) - The M6 forecasting competition.pdf}
}

@article{malkielReflectionsEfficientMarket2005,
  title = {Reflections on the {{Efficient Market Hypothesis}}: 30 {{Years Later}}},
  shorttitle = {Reflections on the {{Efficient Market Hypothesis}}},
  author = {Malkiel, Burton G.},
  year = {2005},
  journal = {Financial Review},
  volume = {40},
  number = {1},
  pages = {1--9},
  issn = {1540-6288},
  doi = {10.1111/j.0732-8516.2005.00090.x},
  urldate = {2023-06-23},
  abstract = {In recent years financial economists have increasingly questioned the efficient market hypothesis. But surely if market prices were often irrational and if market returns were as predictable as some critics have claimed, then professionally managed investment funds should easily be able to outdistance a passive index fund. This paper shows that professional investment managers, both in The U.S. and abroad, do not outperform their index benchmarks and provides evidence that by and large market prices do seem to reflect all available information.},
  langid = {english},
  keywords = {M6,reference},
  file = {C:\Users\stane\Zotero\storage\ZBW5KP4N\Malkiel (2005) - Reflections on the Efficient Market Hypothesis.pdf}
}

@article{mcfaddenMethodSimulatedMoments1989,
  title = {A {{Method}} of {{Simulated Moments}} for {{Estimation}} of {{Discrete Response Models Without Numerical Integration}}},
  author = {McFadden, Daniel},
  year = {1989},
  journal = {Econometrica},
  volume = {57},
  number = {5},
  eprint = {1913621},
  eprinttype = {jstor},
  pages = {995--1026},
  publisher = {{[Wiley, Econometric Society]}},
  issn = {0012-9682},
  doi = {10.2307/1913621},
  urldate = {2023-06-23},
  abstract = {This paper proposes a simple modification of a conventional method of moments estimator for a discrete response model, replacing response probabilities that require numerical integration with estimators obtained by Monte Carlo simulation. This method of simulated moments (MSM) does not require precise estimates of these probabilities for consistency and asymptotic normality, relying instead on the law of large numbers operating across observations to control simulation error, and hence can use simulations of practical size. The method is useful for models such as high-dimensional multinomial probit (MNP), where computation has restricted applications.},
  keywords = {M6,reference},
  file = {C:\Users\stane\Zotero\storage\QLDF7ZVA\McFadden (1989) - A Method of Simulated Moments for Estimation of Discrete Response Models Without Numerical Integration.pdf}
}

@misc{michausFinQBoostMachineLearning2023,
  title = {{{FinQBoost}}: {{Machine Learning}} for {{Portfolio Forecasting}}},
  shorttitle = {{{FinQBoost}}},
  author = {Michaus, Miguel Perez},
  year = {2023},
  month = mar,
  journal = {Medium},
  urldate = {2023-06-09},
  abstract = {The approach behind twin \#2 global prizes in the M6 Financial Forecasting Competition.},
  howpublished = {https://miguelpmich.medium.com/finqboost-machine-learning-for-portfolio-forecasting-55e62b00ebca},
  langid = {english},
  keywords = {M6,read}
}

@misc{micropredictionOptionsMarketBeat2023,
  title = {The {{Options Market Beat}} 94\% of {{Participants}} in the {{M6 Financial Forecasting Contest}}},
  author = {Microprediction},
  year = {2023},
  month = feb,
  journal = {Geek Culture},
  urldate = {2023-02-06},
  abstract = {As of today I have permission to make public my mischievous entry in the year-long world-wide stock and ETF forecasting contest~{\dots} the{\dots}},
  howpublished = {https://medium.com/geekculture/the-options-market-beat-94-of-participants-in-the-m6-financial-forecasting-contest-fa4f47f57d33},
  langid = {english},
  keywords = {M6,reference}
}

@article{pawlikowskiWeightedEnsembleStatistical2020,
  title = {Weighted Ensemble of Statistical Models},
  author = {Pawlikowski, Maciej and Chorowska, Agata},
  year = {2020},
  month = jan,
  journal = {International Journal of Forecasting},
  series = {M4 {{Competition}}},
  volume = {36},
  number = {1},
  pages = {93--97},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2019.03.019},
  urldate = {2023-07-20},
  abstract = {We present a detailed description of our submission for the M4 forecasting competition, in which it ranked 3rd overall. Our solution utilizes several commonly used statistical models, which are weighted according to their performance on historical data. We cluster series within each type of frequency with respect to the existence of trend and seasonality. Every class of series is assigned a different set of models to combine. Combination weights are chosen separately for each series. We conduct experiments with a holdout set to manually pick pools of models that perform best for a given series type, as well as to choose the combination approaches.},
  langid = {english},
  keywords = {M6,skimmed},
  file = {C:\Users\stane\Zotero\storage\WLB3VJNF\Pawlikowski and Chorowska - 2020 - Weighted ensemble of statistical models.pdf}
}

@book{seberMultivariateObservations1984,
  title = {Multivariate {{Observations}}},
  author = {Seber, George A. F.},
  year = {1984},
  month = apr,
  edition = {1st edition},
  publisher = {{Wiley}},
  address = {{New York}},
  abstract = {WILEY-INTERSCIENCE PAPERBACK SERIESThe Wiley-Interscience Paperback Series consists of selected books that have been made more accessible to consumers in an effort to increase global appeal and general circulation. With these new unabridged softcover volumes, Wiley hopes to extend the lives of these works by making them available to future generations of statisticians, mathematicians, and scientists."In recent years many monographs have been published on specialized aspects of multivariate data-analysis{\textendash}on cluster analysis, multidimensional scaling, correspondence analysis, developments of discriminant analysis, graphical methods, classification, and so on. This book is an attempt to review these newer methods together with the classical theory. . . . This one merits two cheers." {\textendash}J. C. Gower, Department of Statistics Rothamsted Experimental Station, Harpenden, U.K. Review in Biometrics, June 1987Multivariate Observations is a comprehensive sourcebook that treats data-oriented techniques as well as classical methods. Emphasis is on principles rather than mathematical detail, and coverage ranges from the practical problems of graphically representing high-dimensional data to the theoretical problems relating to matrices of random variables. Each chapter serves as a self-contained survey of a specific topic. The book includes many numerical examples and over 1,100 references.},
  isbn = {978-0-471-88104-9},
  langid = {english},
  keywords = {M6,reference},
  file = {C:\Users\stane\Zotero\storage\46CZM9VK\Seber - 1984 - Multivariate Observations.pdf}
}

@article{semenoglouInvestigatingAccuracyCrosslearning2021,
  title = {Investigating the Accuracy of Cross-Learning Time Series Forecasting Methods},
  author = {Semenoglou, Artemios-Anargyros and Spiliotis, Evangelos and Makridakis, Spyros and Assimakopoulos, Vassilios},
  year = {2021},
  month = jul,
  journal = {International Journal of Forecasting},
  volume = {37},
  number = {3},
  pages = {1072--1084},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2020.11.009},
  urldate = {2024-01-22},
  abstract = {The M4 competition identified innovative forecasting methods, advancing the theory and practice of forecasting. One of the most promising innovations of M4 was the utilization of cross-learning approaches that allow models to learn from multiple series how to accurately predict individual ones. In this paper, we investigate the potential of cross-learning by developing various neural network models that adopt such an approach, and we compare their accuracy to that of traditional models that are trained in a series-by-series fashion. Our empirical evaluation, which is based on the M4 monthly data, confirms that cross-learning is a promising alternative to traditional forecasting, at least when appropriate strategies for extracting information from large, diverse time series data sets are considered. Ways of combining traditional with cross-learning methods are also examined in order to initiate further research in the field.},
  keywords = {M6,out,waiting},
  file = {C:\Users\stane\Zotero\storage\EUDFQ9WG\Semenoglou et al (2021) - Investigating the accuracy of cross-learning time series forecasting methods.pdf}
}

@inproceedings{smylDataPreprocessingAugmentation2016,
  title = {Data Preprocessing and Augmentation for Multiple Short Time Series Forecasting with Recurrent Neural Networks},
  booktitle = {36th International Symposium on Forecasting},
  author = {Smyl, Slawek and Kuber, Karthik},
  year = {2016},
  urldate = {2024-01-29},
  keywords = {M6,out,reference},
  file = {C:\Users\stane\Zotero\storage\VQL3F67K\Smyl, Kuber (2016) - Data preprocessing and augmentation for multiple short time series forecasting with recurrent neural networks.pdf}
}

@article{smylHybridMethodExponential2020,
  title = {A Hybrid Method of Exponential Smoothing and Recurrent Neural Networks for Time Series Forecasting},
  author = {Smyl, Slawek},
  year = {2020},
  month = jan,
  journal = {International Journal of Forecasting},
  series = {M4 {{Competition}}},
  volume = {36},
  number = {1},
  pages = {75--85},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2019.03.017},
  urldate = {2020-12-30},
  abstract = {This paper presents the winning submission of the M4 forecasting competition. The submission utilizes a dynamic computational graph neural network system that enables a standard exponential smoothing model to be mixed with advanced long short term memory networks into a common framework. The result is a hybrid and hierarchical forecasting method.},
  langid = {english},
  keywords = {M6,out,skimmed},
  file = {C\:\\Users\\stane\\Zotero\\storage\\VI7AQQIC\\Smyl (2020) - A hybrid method of exponential smoothing and recurrent neural networks for time series forecasting.pdf;C\:\\Users\\stane\\Zotero\\storage\\4VZB27UP\\S0169207019301153.html}
}

@misc{stanekNoteM6Forecasting2023,
  type = {{{SSRN Scholarly Paper}}},
  title = {A {{Note}} on the {{M6 Forecasting Competition}}: {{Designing Parametric Models}} with {{Hypernetworks}}},
  shorttitle = {A {{Note}} on the {{M6 Forecasting Competition}}},
  author = {Stan{\v e}k, Filip},
  year = {2023},
  month = feb,
  number = {4355794},
  address = {{Rochester, NY}},
  doi = {10.2139/ssrn.4355794},
  urldate = {2023-08-21},
  abstract = {This short note outlines the general approach used for the forecasting part of the M6 forecasting competition. It describes a meta-learning approach that is based on an encoder-decoder hypernetwork, capable of identifying the most appropriate parametric model for a given family of related prediction tasks. In addition to its application in the M6 forecasting competition, we also evaluate it on the sinusoidal regression problem. There, the proposed method outperforms established methods by an order of magnitude, achieving near-oracle level performance.},
  langid = {english},
  keywords = {idea,M6},
  file = {C:\Users\stane\Zotero\storage\D6KWTPEM\Staněk (2023) - A Note on the M6 Forecasting Competition.pdf}
}

@misc{stanekNoteM6Forecasting2023a,
  type = {{{SSRN Scholarly Paper}}},
  title = {A {{Note}} on the {{M6 Forecasting Competition}}: {{Rank Optimization}}},
  shorttitle = {A {{Note}} on the {{M6 Forecasting Competition}}},
  author = {Stan{\v e}k, Filip},
  year = {2023},
  month = jul,
  number = {4527154},
  address = {{Rochester, NY}},
  doi = {10.2139/ssrn.4527154},
  urldate = {2023-10-01},
  abstract = {We describe a strategy applicable to the investment part of the M6 Forecasting Competition, which maximizes the probability of securing at least the top q-th rank. This portfolio strategy can attain a comparable probability of winning as a participant capable of consistently generating approximately double the market returns. However, it exhibits poor performance in expectation. This highlights that the task of succeeding in such a competition may not always coincide with the task of maximizing expected investment returns.},
  langid = {english},
  keywords = {idea,M6},
  file = {C:\Users\stane\Zotero\storage\BJEDJVPC\Staněk (2023) - A Note on the M6 Forecasting Competition.pdf}
}

@misc{ulrichTTRTechnicalTrading2021,
  title = {{{TTR}}: {{Technical Trading Rules}}},
  shorttitle = {{{TTR}}},
  author = {Ulrich, Joshua},
  year = {2021},
  month = dec,
  urldate = {2022-12-06},
  abstract = {A collection of over 50 technical indicators for creating technical trading rules. The package also provides fast implementations of common rolling-window functions, and several volatility calculations.},
  copyright = {GPL-2 {\textbar} GPL-3 [expanded from: GPL ({$\geq$} 2)]},
  keywords = {M6,reference}
}

@misc{webster500Returns19302023,
  title = {S\&{{P}} 500 {{Returns}} since 1930},
  author = {Webster, Ian},
  year = {2023},
  urldate = {2023-05-09},
  howpublished = {https://www.officialdata.org/us/stocks/s-p-500/1930},
  langid = {english},
  keywords = {M6,reference}
}

@article{yeImplementingTransferLearning2021,
  title = {Implementing Transfer Learning across Different Datasets for Time Series Forecasting},
  author = {Ye, Rui and Dai, Qun},
  year = {2021},
  month = jan,
  journal = {Pattern Recognition},
  volume = {109},
  pages = {107617},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2020.107617},
  urldate = {2024-01-22},
  abstract = {Due to the extensive practical value of time series prediction, many excellent algorithms have been proposed. Most of these methods are developed assuming that massive labeled training data are available. However, this assumption might be invalid in some actual situations. To address this limitation, a transfer learning framework with deep architectures is proposed. Since convolutional neural network (CNN) owns favorable feature extraction capability and can implement parallelization more easily, we propose a deep transfer learning method resorting to the architecture of CNN, termed as DTr-CNN for short. It can effectively alleviate the available labeled data absence and leverage useful knowledge to the current prediction. Notably, in our method, transfer learning process is implemented across different datasets. For a given target domain, in real-world scenarios, relativity of truly available potential source datasets may not be obvious, which is challenging and rarely referred to in most existing time series prediction methods. Aiming at this problem, the incorporation of Dynamic Time Warping (DTW) and Jensen-Shannon (JS) divergence is adopted for the selection of the appropriate source domain. Effectiveness of the proposed method is empirically underpinned by the experiments conducted on one group of synthetic and two groups of practical datasets. Besides, an additional experiment on NN5 dataset is conducted.},
  keywords = {M6,out,waiting},
  file = {C:\Users\stane\Zotero\storage\FCP84D2Q\Ye, Dai (2021) - Implementing transfer learning across different datasets for time series forecasting.pdf}
}

@article{zhangSurveyMultiTaskLearning2022,
  title = {A {{Survey}} on {{Multi-Task Learning}}},
  author = {Zhang, Yu and Yang, Qiang},
  year = {2022},
  month = dec,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {34},
  number = {12},
  pages = {5586--5609},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2021.3070203},
  urldate = {2024-02-05},
  abstract = {Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL from the perspective of algorithmic modeling, applications and theoretical analyses. For algorithmic modeling, we give a definition of MTL and then classify different MTL algorithms into five categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach and decomposition approach as well as discussing the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, we review online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works in this paper. Finally, we present theoretical analyses and discuss several future directions for MTL.},
  keywords = {M6,reference},
  file = {C:\Users\stane\Zotero\storage\SA7YBS2D\Zhang, Yang (2022) - A Survey on Multi-Task Learning.pdf}
}

@article{hyndmanAutomaticTimeSeries2008a,
  title = {Automatic {{Time Series Forecasting}}: {{The}} Forecast {{Package}} for {{R}}},
  shorttitle = {Automatic {{Time Series Forecasting}}},
  author = {Hyndman, Rob J. and Khandakar, Yeasmin},
  year = {2008},
  month = jul,
  journal = {Journal of Statistical Software},
  volume = {27},
  pages = {1--22},
  issn = {1548-7660},
  doi = {10.18637/jss.v027.i03},
  urldate = {2024-07-01},
  abstract = {Automatic forecasts of large numbers of univariate time series are often needed in business and other contexts. We describe two automatic forecasting algorithms that have been implemented in the forecast package for R. The first is based on innovations state space models that underly exponential smoothing methods. The second is a step-wise algorithm for forecasting with ARIMA models. The algorithms are applicable to both seasonal and non-seasonal data, and are compared and illustrated using four real time series. We also briefly describe some of the other functionality available in the forecast package.},
  copyright = {Copyright (c) 2007 Rob J. Hyndman, Yeasmin Khandakar},
  langid = {english},
  keywords = {M6,reference},
  file = {C:\Users\stane\Zotero\storage\W67TU2CW\Hyndman, Khandakar (2008) - Automatic Time Series Forecasting.pdf}
}

@article{hyndmanStateSpaceFramework2002a,
  title = {A State Space Framework for Automatic Forecasting Using Exponential Smoothing Methods},
  author = {Hyndman, Rob J and Koehler, Anne B and Snyder, Ralph D and Grose, Simone},
  year = {2002},
  month = jul,
  journal = {International Journal of Forecasting},
  volume = {18},
  number = {3},
  pages = {439--454},
  issn = {0169-2070},
  doi = {10.1016/S0169-2070(01)00110-8},
  urldate = {2024-07-01},
  abstract = {We provide a new approach to automatic forecasting based on an extended range of exponential smoothing methods. Each method in our taxonomy of exponential smoothing methods provides forecasts that are equivalent to forecasts from a state space model. This equivalence allows: (1) easy calculation of the likelihood, the AIC and other model selection criteria; (2) computation of prediction intervals for each method; and (3) random simulation from the underlying state space model. We demonstrate the methods by applying them to the data from the M-competition and the M3-competition. The method provides forecast accuracy comparable to the best methods in the competitions; it is particularly good for short forecast horizons with seasonal data.},
  keywords = {M6,reference},
  file = {C:\Users\stane\Zotero\storage\CHVPUDLR\Hyndman et al (2002) - A state space framework for automatic forecasting using exponential smoothing methods.pdf}
}

@article{izenmanReducedrankRegressionMultivariate1975,
  title = {Reduced-Rank Regression for the Multivariate Linear Model},
  author = {Izenman, Alan Julian},
  year = {1975},
  month = jun,
  journal = {Journal of Multivariate Analysis},
  volume = {5},
  number = {2},
  pages = {248--264},
  issn = {0047-259X},
  doi = {10.1016/0047-259X(75)90042-1},
  urldate = {2024-06-24},
  abstract = {The problem of estimating the regression coefficient matrix having known (reduced) rank for the multivariate linear model when both sets of variates are jointly stochastic is discussed. We show that this problem is related to the problem of deciding how many principal components or pairs of canonical variates to use in any practical situation. Under the assumption of joint normality of the two sets of variates, we give the asymptotic (large-sample) distributions of the various estimated reduced-rank regression coefficient matrices that are of interest. Approximate confidence bounds on the elements of these matrices are then suggested using either the appropriate asymptotic expressions or the jackknife technique.},
  keywords = {M6,reference},
  file = {C:\Users\stane\Zotero\storage\PVTNURFG\Izenman (1975) - Reduced-rank regression for the multivariate linear model.pdf}
}

@misc{leeTimeSeriesForecasting2022,
  title = {Time {{Series Forecasting}} with {{Hypernetworks Generating Parameters}} in {{Advance}}},
  author = {Lee, Jaehoon and Kim, Chan and Lee, Gyumin and Lim, Haksoo and Choi, Jeongwhan and Lee, Kookjin and Lee, Dongeun and Hong, Sanghyun and Park, Noseong},
  year = {2022},
  month = nov,
  number = {arXiv:2211.12034},
  eprint = {2211.12034},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.12034},
  urldate = {2024-04-07},
  abstract = {Forecasting future outcomes from recent time series data is not easy, especially when the future data are different from the past (i.e. time series are under temporal drifts). Existing approaches show limited performances under data drifts, and we identify the main reason: It takes time for a model to collect sufficient training data and adjust its parameters for complicated temporal patterns whenever the underlying dynamics change. To address this issue, we study a new approach; instead of adjusting model parameters (by continuously re-training a model on new data), we build a hypernetwork that generates other target models' parameters expected to perform well on the future data. Therefore, we can adjust the model parameters beforehand (if the hypernetwork is correct). We conduct extensive experiments with 6 target models, 6 baselines, and 4 datasets, and show that our HyperGPA outperforms other baselines.},
  archiveprefix = {arXiv},
  keywords = {M6,read,reflection},
  file = {C:\Users\stane\Zotero\storage\5Y7NHRV9\Lee et al (2022) - Time Series Forecasting with Hypernetworks Generating Parameters in Advance.pdf}
}

@inproceedings{lake2011one,
  title = {One Shot Learning of Simple Visual Concepts},
  booktitle = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  author = {Lake, Brenden and Salakhutdinov, Ruslan and Gross, Jason and Tenenbaum, Joshua},
  year = {2011},
  volume = {33},
  keywords = {M6,reference}
}

@inproceedings{ravi2016optimization,
  title = {Optimization as a Model for Few-Shot Learning},
  booktitle = {International Conference on Learning Representations},
  author = {Ravi, Sachin and Larochelle, Hugo},
  year = {2016},
  keywords = {M6,reference}
}

@article{makridakisM4Competition1002020,
  title = {The {{M4 Competition}}: 100,000 Time Series and 61 Forecasting Methods},
  shorttitle = {The {{M4 Competition}}},
  author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
  year = {2020},
  month = jan,
  journal = {International Journal of Forecasting},
  series = {M4 {{Competition}}},
  volume = {36},
  number = {1},
  pages = {54--74},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2019.04.014},
  urldate = {2020-12-29},
  abstract = {The M4 Competition follows on from the three previous M competitions, the purpose of which was to learn from empirical evidence both how to improve the forecasting accuracy and how such learning could be used to advance the theory and practice of forecasting. The aim of M4 was to replicate and extend the three previous competitions by: (a) significantly increasing the number of series, (b) expanding the number of forecasting methods, and (c) including prediction intervals in the evaluation process as well as point forecasts. This paper covers all aspects of M4 in detail, including its organization and running, the presentation of its results, the top-performing methods overall and by categories, its major findings and their implications, and the computational requirements of the various methods. Finally, it summarizes its main conclusions and states the expectation that its series will become a testing ground for the evaluation of new methods and the improvement of the practice of forecasting, while also suggesting some ways forward for the field.},
  langid = {english},
  keywords = {CERGE-EI,skimmed},
  file = {C\:\\Users\\stane\\Zotero\\storage\\ZSXXCUZ2\\Makridakis et al (2020) - The M4 Competition.pdf;C\:\\Users\\stane\\Zotero\\storage\\YFMFZ2EV\\S0169207019301128.html}
}

@article{hadiCautionaryNotesUse1998,
  title = {Some {{Cautionary Notes}} on the {{Use}} of {{Principal Components Regression}}},
  author = {Hadi, Ali S. and Ling, Robert F.},
  year = {1998},
  month = feb,
  journal = {The American Statistician},
  volume = {52},
  number = {1},
  pages = {15--19},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.1998.10480530},
  urldate = {2024-06-30},
  abstract = {Many textbooks on regression analysis include the methodology of principal components regression (PCR) as a way of treating multicollinearity problems. Although we have not encountered any strong justification of the methodology, we have encountered, through carrying out the methodology in well-known data sets with severe multicollinearity, serious actual and potential pitfalls in the methodology. We address these pitfalls as cautionary notes, numerical examples that use well-known data sets. We also illustrate by theory and example that it is possible for the PCR to fail miserably in the sense that when the response variable is regressed on all of the p principal components (PCs), the first (p - 1) PCs contribute nothing toward the reduction of the residual sum of squares, yet the last PC alone (the one that is always discarded according to PCR methodology) contributes everything. We then give conditions under which the PCR totally fails in the above sense.},
  keywords = {M6,reference},
  file = {C:\Users\stane\Zotero\storage\S78QHDFU\Hadi, Ling (1998) - Some Cautionary Notes on the Use of Principal Components Regression.pdf}
}

@misc{hyndmanTsfeaturesTimeSeries2023,
  title = {Tsfeatures: {{Time Series Feature Extraction}}},
  shorttitle = {Tsfeatures},
  author = {Hyndman, Rob and {Montero-Manso}, Pablo and {O'Hara-Wild}, Mitchell and Talagala, Thiyanga and Wang, Earo and Yang, Yangzhuoran and Taieb, Souhaib Ben and Hanqing, Cao and Lake, D. K. and Laptev, Nikolay and Moorman, J. R. and Zhang, Bohan},
  year = {2023},
  month = aug,
  urldate = {2024-07-01},
  abstract = {Methods for extracting various features from time series data. The features provided are those from Hyndman, Wang and Laptev (2013) {$<$}doi:10.1109/ICDMW.2015.104{$>$}, Kang, Hyndman and Smith-Miles (2017) {$<$}doi:10.1016/j.ijforecast.2016.09.004{$>$} and from Fulcher, Little and Jones (2013) {$<$}doi:10.1098/rsif.2013.0048{$>$}. Features include spectral entropy, autocorrelations, measures of the strength of seasonality and trend, and so on. Users can also define their own feature functions.},
  copyright = {GPL-3},
  keywords = {M6,reference}
}